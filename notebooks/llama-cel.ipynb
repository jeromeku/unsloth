{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import LlamaTokenizer, AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.models.llama import LlamaForCausalLM, LlamaConfig\n",
    "from transformers import BitsAndBytesConfig\n",
    "import unsloth.memory_utils as memory_utils\n",
    "import unsloth.data_utils as data_utils\n",
    "from unsloth.kernels import fused_cel\n",
    "from unsloth.models._utils import patch_tokenizer\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/notebooks/conda/envs/unsloth/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39e3031d0aa249538580e212712605eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model_config = LlamaConfig.from_pretrained(\"./llama-10m.json\")\n",
    "# ref_model = LlamaForCausalLM(model_config).to(\"cuda\")\n",
    "small_model = LlamaForCausalLM(model_config).to(\"cuda\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\", quantization_config=quant_config\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\", model_max_length=4096, padding_side=\"right\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "meta-llama/Llama-2-7b-chat-hf does not have a padding token! Will use pad_token = <unk>.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = patch_tokenizer(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs, seqlen, in_features = 1, 16, 4096\n",
    "\n",
    "input_ids = torch.randint(0, model.config.vocab_size, (bs, seqlen), device=\"cuda\")\n",
    "labels = input_ids.detach().clone()\n",
    "attention_mask = torch.ones((bs, seqlen), device=\"cuda\")\n",
    "\n",
    "ref_out = model(input_ids, labels=labels, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:unsloth.kernels.fused_cel:Using fused cross entropy loss, output logits will be in None\n"
     ]
    }
   ],
   "source": [
    "from unsloth.kernels.fused_cel import patch_model as patch_model_fused_cel\n",
    "\n",
    "fused_model = patch_model_fused_cel(model, use_fused_cel=True)\n",
    "fused_out = fused_model(input_ids, labels=labels, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(12.6158, device='cuda:0', grad_fn=<NllLossBackward0>),\n",
       " tensor(12.6158, device='cuda:0',\n",
       "        grad_fn=<FusedCrossEntropyLossFunctionBackward>))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(torch.float32, torch.float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_out.loss, fused_out.loss\n",
    "ref_out.loss.dtype, fused_out.loss.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CausalLMOutputWithPast(loss=tensor(10.3616, device='cuda:0',\n",
       "       grad_fn=<FusedCrossEntropyLossFunctionBackward>), logits=None, past_key_values=((tensor([[[[ 0.1497,  0.1464,  0.0359,  ..., -0.2845,  0.1901, -0.0329],\n",
       "          [ 0.1350,  0.2444, -0.0649,  ...,  0.2775, -0.5182, -0.0302],\n",
       "          [-0.1150, -0.0163, -0.4294,  ...,  0.1441, -0.4375,  0.3417],\n",
       "          ...,\n",
       "          [-0.0815, -0.1239,  0.0535,  ..., -0.1271, -0.1182,  0.2920],\n",
       "          [ 0.0310, -0.1295,  0.1272,  ...,  0.0997, -0.1399,  0.0415],\n",
       "          [ 0.4750,  0.3073, -0.0646,  ...,  0.0314,  0.0284,  0.1492]],\n",
       "\n",
       "         [[-0.3523,  0.0220,  0.1917,  ...,  0.0104, -0.2017,  0.6086],\n",
       "          [-0.2150,  0.1729, -0.0763,  ...,  0.1541, -0.1428, -0.3597],\n",
       "          [-0.0369,  0.0356,  0.1930,  ..., -0.0281, -0.3698, -0.2495],\n",
       "          ...,\n",
       "          [-0.1349,  0.1994,  0.0579,  ...,  0.3320, -0.1827,  0.1457],\n",
       "          [-0.1372, -0.0760,  0.2372,  ...,  0.0172,  0.0316, -0.0693],\n",
       "          [ 0.1217,  0.4477, -0.3213,  ..., -0.3093, -0.1079,  0.0862]],\n",
       "\n",
       "         [[-0.4753,  0.0469, -0.3004,  ...,  0.0250,  0.1189, -0.0131],\n",
       "          [-0.2186, -0.2416, -0.1491,  ..., -0.1982,  0.1001,  0.2782],\n",
       "          [-0.0682,  0.1609, -0.0469,  ..., -0.1091,  0.0441,  0.1307],\n",
       "          ...,\n",
       "          [ 0.0160, -0.1580, -0.0317,  ..., -0.2040, -0.2026, -0.0189],\n",
       "          [-0.0127, -0.0827, -0.2459,  ...,  0.1234, -0.0390, -0.0772],\n",
       "          [ 0.1335,  0.2215,  0.1892,  ..., -0.0512, -0.1653,  0.0604]],\n",
       "\n",
       "         [[-0.2039, -0.0056,  0.2550,  ..., -0.0433, -0.2389, -0.0952],\n",
       "          [ 0.4591, -0.2520,  0.1569,  ...,  0.3836, -0.2183, -0.3881],\n",
       "          [ 0.1929,  0.0217,  0.2350,  ..., -0.1253,  0.2295,  0.2031],\n",
       "          ...,\n",
       "          [ 0.3010, -0.2225, -0.0401,  ..., -0.2000,  0.2137,  0.0621],\n",
       "          [ 0.1840, -0.2119,  0.0706,  ..., -0.1441, -0.1623, -0.3247],\n",
       "          [-0.6384,  0.0205,  0.0305,  ...,  0.3508,  0.0692,  0.0612]]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>), tensor([[[[-0.2017, -0.4194,  0.1531,  ..., -0.3401, -0.3379, -0.2122],\n",
       "          [-0.0291,  0.0546,  0.1850,  ..., -0.2846, -0.0638,  0.0432],\n",
       "          [ 0.2375, -0.2129, -0.1464,  ..., -0.1777, -0.2110, -0.5032],\n",
       "          ...,\n",
       "          [ 0.2202,  0.1251, -0.2830,  ..., -0.1808,  0.0636,  0.4438],\n",
       "          [ 0.1443,  0.0048, -0.2038,  ..., -0.2012,  0.2627, -0.3071],\n",
       "          [-0.0246, -0.0625, -0.0093,  ...,  0.0555,  0.0998,  0.0928]],\n",
       "\n",
       "         [[-0.3477,  0.1449,  0.1058,  ...,  0.2087,  0.0289, -0.5707],\n",
       "          [ 0.0940,  0.1937,  0.1676,  ..., -0.1479,  0.2995,  0.3241],\n",
       "          [-0.1268,  0.1355,  0.1199,  ..., -0.0153, -0.0957, -0.0888],\n",
       "          ...,\n",
       "          [ 0.0181,  0.0382, -0.0961,  ...,  0.4339, -0.3005,  0.1609],\n",
       "          [ 0.1181,  0.4148, -0.1621,  ...,  0.0136,  0.1425, -0.0225],\n",
       "          [ 0.0211, -0.2603,  0.2303,  ...,  0.0196, -0.0393,  0.0534]],\n",
       "\n",
       "         [[ 0.1274, -0.1343,  0.1580,  ..., -0.3838, -0.1359,  0.3091],\n",
       "          [-0.3016,  0.2830,  0.4128,  ...,  0.2631, -0.1246, -0.0352],\n",
       "          [-0.3638,  0.3189, -0.3190,  ..., -0.1930, -0.0756, -0.0479],\n",
       "          ...,\n",
       "          [-0.1563, -0.0300, -0.0425,  ...,  0.4354,  0.1437,  0.0553],\n",
       "          [-0.5685,  0.1283,  0.0756,  ..., -0.1224, -0.1260, -0.0721],\n",
       "          [ 0.3470, -0.1257,  0.2240,  ...,  0.0035, -0.0046, -0.4528]],\n",
       "\n",
       "         [[ 0.3727,  0.1364,  0.3874,  ...,  0.0470, -0.0766, -0.1441],\n",
       "          [-0.0097,  0.0177, -0.0819,  ..., -0.0948,  0.4124, -0.1969],\n",
       "          [ 0.1183,  0.1556,  0.2740,  ...,  0.0026, -0.3448,  0.2513],\n",
       "          ...,\n",
       "          [-0.1962,  0.0686, -0.2320,  ..., -0.1597,  0.0409, -0.4576],\n",
       "          [-0.3458, -0.0838, -0.1105,  ..., -0.2813,  0.1110,  0.1109],\n",
       "          [-0.0629,  0.1438,  0.3279,  ..., -0.3140,  0.1008, -0.1223]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>)), (tensor([[[[ 0.4292,  0.2323, -0.3899,  ...,  0.1455,  0.0625, -0.0559],\n",
       "          [-0.0359,  0.3092, -0.4750,  ..., -0.1850, -0.0291,  0.0670],\n",
       "          [-0.4733,  0.1328, -0.1051,  ...,  0.0467,  0.3019, -0.0421],\n",
       "          ...,\n",
       "          [ 0.3568, -0.0417, -0.3964,  ..., -0.3007, -0.1547,  0.0048],\n",
       "          [ 0.2123,  0.1000, -0.4664,  ..., -0.0080, -0.2207, -0.1065],\n",
       "          [-0.0539, -0.4225, -0.4793,  ...,  0.1129, -0.3168, -0.1583]],\n",
       "\n",
       "         [[-0.1913,  0.2254,  0.0200,  ..., -0.1980, -0.1737,  0.1141],\n",
       "          [-0.1118,  0.1495, -0.4699,  ..., -0.3053,  0.0111, -0.0937],\n",
       "          [-0.2313,  0.1768, -0.0858,  ..., -0.2758, -0.3677, -0.1811],\n",
       "          ...,\n",
       "          [ 0.0872,  0.0036, -0.0621,  ...,  0.2156, -0.2166, -0.0161],\n",
       "          [ 0.2809, -0.0245,  0.1851,  ...,  0.3475,  0.0438, -0.0411],\n",
       "          [-0.2152,  0.0846,  0.0659,  ...,  0.0774,  0.3985,  0.2325]],\n",
       "\n",
       "         [[-0.6583, -0.1347,  0.3397,  ...,  0.1645,  0.0099, -0.4496],\n",
       "          [ 0.0067, -0.3705,  0.3653,  ...,  0.2991,  0.1432,  0.4717],\n",
       "          [ 0.1798, -0.2214,  0.6944,  ...,  0.1664,  0.3498, -0.3632],\n",
       "          ...,\n",
       "          [ 0.1533, -0.0968,  0.0637,  ..., -0.2248, -0.2776,  0.4163],\n",
       "          [ 0.1326,  0.0853,  0.0272,  ...,  0.0602, -0.1774,  0.1252],\n",
       "          [-0.0096, -0.0220, -0.2565,  ..., -0.0805,  0.0544,  0.0562]],\n",
       "\n",
       "         [[-0.6488,  0.3066,  0.0781,  ...,  0.1305, -0.3636,  0.1671],\n",
       "          [-0.1496,  0.2003, -0.2754,  ...,  0.3205, -0.0126,  0.2796],\n",
       "          [ 0.4365, -0.0146,  0.0797,  ...,  0.4025,  0.0653, -0.1115],\n",
       "          ...,\n",
       "          [-0.3501,  0.0715, -0.2027,  ..., -0.0010,  0.1424,  0.2301],\n",
       "          [-0.2104,  0.4029,  0.1098,  ..., -0.2036,  0.3409,  0.1301],\n",
       "          [-0.0532,  0.0206,  0.2394,  ..., -0.0388, -0.1160,  0.1750]]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>), tensor([[[[ 0.1055,  0.3487,  0.2135,  ..., -0.0069,  0.0662,  0.2025],\n",
       "          [ 0.1744,  0.3916,  0.1858,  ..., -0.0804, -0.0978, -0.2125],\n",
       "          [-0.1229,  0.3148,  0.1857,  ..., -0.1679, -0.1809,  0.0525],\n",
       "          ...,\n",
       "          [-0.0728,  0.0452,  0.0495,  ..., -0.0365,  0.1844,  0.1022],\n",
       "          [ 0.0406,  0.0765, -0.0814,  ...,  0.0669,  0.4303, -0.0359],\n",
       "          [ 0.1411,  0.3990,  0.1808,  ..., -0.0024, -0.2703, -0.1214]],\n",
       "\n",
       "         [[-0.1368, -0.2627,  0.1068,  ..., -0.2890,  0.2155, -0.3107],\n",
       "          [ 0.0491, -0.1309, -0.1061,  ..., -0.1555,  0.1804, -0.0647],\n",
       "          [ 0.1757,  0.0459,  0.0222,  ..., -0.4544,  0.0523, -0.1479],\n",
       "          ...,\n",
       "          [-0.3627, -0.1628,  0.0265,  ...,  0.3232,  0.1377, -0.1433],\n",
       "          [-0.2703, -0.0218,  0.2321,  ...,  0.0579, -0.0916, -0.2139],\n",
       "          [ 0.0810,  0.0197, -0.3852,  ..., -0.2918,  0.3794,  0.2730]],\n",
       "\n",
       "         [[-0.4636,  0.1724, -0.1955,  ...,  0.1042, -0.4980, -0.1748],\n",
       "          [-0.1426, -0.3868, -0.3378,  ...,  0.1094, -0.5606, -0.1907],\n",
       "          [-0.0079,  0.3641, -0.0477,  ..., -0.0358, -0.6029, -0.3258],\n",
       "          ...,\n",
       "          [ 0.2728, -0.2014, -0.2296,  ..., -0.3484, -0.0275, -0.4144],\n",
       "          [ 0.1712,  0.1092, -0.0198,  ..., -0.2603, -0.1107, -0.2091],\n",
       "          [ 0.0594, -0.1814,  0.0399,  ..., -0.2849, -0.4324, -0.0072]],\n",
       "\n",
       "         [[-0.2638, -0.5052,  0.5460,  ...,  0.3130,  0.2723, -0.2954],\n",
       "          [-0.2788, -0.3218,  0.2426,  ...,  0.3033,  0.4037,  0.2860],\n",
       "          [-0.1505,  0.0271,  0.1004,  ...,  0.4232,  0.2300, -0.1981],\n",
       "          ...,\n",
       "          [-0.2499, -0.1992,  0.2652,  ...,  0.0473,  0.3052, -0.0139],\n",
       "          [ 0.0465, -0.0745,  0.2458,  ...,  0.1952,  0.1511,  0.1029],\n",
       "          [-0.3297,  0.0206, -0.1769,  ...,  0.2343, -0.0595, -0.2281]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>)), (tensor([[[[ 0.0320, -0.0386,  0.2228,  ...,  0.1313,  0.1214, -0.0070],\n",
       "          [ 0.3217, -0.1341, -0.0274,  ..., -0.0536, -0.1260, -0.0272],\n",
       "          [-0.0142, -0.0868,  0.3291,  ..., -0.2954, -0.0532, -0.0865],\n",
       "          ...,\n",
       "          [ 0.0419,  0.2890,  0.0758,  ...,  0.0478, -0.4884, -0.1077],\n",
       "          [ 0.1761,  0.3348,  0.0904,  ..., -0.1213, -0.2735,  0.0142],\n",
       "          [-0.0539,  0.0381, -0.0471,  ...,  0.0820, -0.1366, -0.0643]],\n",
       "\n",
       "         [[-0.0060, -0.2686, -0.2110,  ..., -0.2776, -0.0727,  0.0208],\n",
       "          [-0.1702,  0.0875,  0.0714,  ...,  0.0684, -0.1176,  0.2024],\n",
       "          [-0.1499,  0.1658,  0.0818,  ...,  0.0327, -0.1037,  0.0555],\n",
       "          ...,\n",
       "          [-0.0698,  0.2728,  0.3480,  ..., -0.1942, -0.1605, -0.0651],\n",
       "          [ 0.0560,  0.1359,  0.2211,  ...,  0.1617, -0.2396, -0.0208],\n",
       "          [-0.0120,  0.0976,  0.1824,  ...,  0.0874, -0.2250,  0.0277]],\n",
       "\n",
       "         [[-0.2178,  0.1337, -0.3536,  ...,  0.0546, -0.0327,  0.2504],\n",
       "          [ 0.0134, -0.1130, -0.3334,  ..., -0.2106, -0.1079, -0.0129],\n",
       "          [-0.0499,  0.0351, -0.2872,  ..., -0.1921, -0.1644, -0.0543],\n",
       "          ...,\n",
       "          [-0.0524, -0.0079,  0.0760,  ..., -0.3486, -0.1932, -0.2753],\n",
       "          [-0.0309, -0.1535, -0.1042,  ..., -0.1061, -0.0661, -0.3043],\n",
       "          [-0.1485, -0.0864, -0.2300,  ..., -0.3889, -0.0137, -0.1879]],\n",
       "\n",
       "         [[-0.1175, -0.1174,  0.2016,  ..., -0.1040,  0.1834, -0.1515],\n",
       "          [ 0.0548, -0.0924,  0.2378,  ...,  0.0677,  0.1091, -0.0297],\n",
       "          [ 0.0283, -0.1112,  0.0932,  ...,  0.1505,  0.2190,  0.0259],\n",
       "          ...,\n",
       "          [ 0.0715, -0.1427,  0.0146,  ...,  0.2826, -0.0686, -0.0595],\n",
       "          [ 0.2205,  0.1547,  0.0074,  ...,  0.3366, -0.1624,  0.0992],\n",
       "          [ 0.1398,  0.1768,  0.2927,  ...,  0.5268,  0.0086,  0.0651]]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>), tensor([[[[-0.1472, -0.1623, -0.0859,  ..., -0.3290,  0.2158, -0.0524],\n",
       "          [ 0.1646, -0.0281,  0.2358,  ..., -0.0451,  0.0848, -0.0716],\n",
       "          [-0.0298, -0.0796,  0.2579,  ..., -0.0699,  0.0141,  0.1040],\n",
       "          ...,\n",
       "          [ 0.0377,  0.2523,  0.4098,  ...,  0.1204,  0.0036, -0.0968],\n",
       "          [ 0.3129,  0.1270,  0.1427,  ...,  0.2084,  0.0925, -0.1147],\n",
       "          [ 0.2904, -0.1213,  0.2589,  ...,  0.0193, -0.0691, -0.1109]],\n",
       "\n",
       "         [[-0.4852,  0.0462, -0.6559,  ...,  0.1854,  0.1216,  0.4674],\n",
       "          [-0.0365, -0.2516, -0.5463,  ...,  0.1528,  0.1883,  0.5542],\n",
       "          [-0.2207, -0.3726, -0.3279,  ...,  0.0700,  0.3095,  0.4535],\n",
       "          ...,\n",
       "          [ 0.1093, -0.3434, -0.2376,  ...,  0.1462,  0.0701,  0.4423],\n",
       "          [ 0.1359, -0.3169,  0.0525,  ..., -0.2068,  0.0792,  0.4792],\n",
       "          [ 0.0033, -0.2744, -0.2507,  ..., -0.0477,  0.1135,  0.6842]],\n",
       "\n",
       "         [[ 0.3914, -0.5094, -0.2086,  ..., -0.1530,  0.0537, -0.2594],\n",
       "          [ 0.4664, -0.6523, -0.1245,  ...,  0.1412, -0.1910,  0.2979],\n",
       "          [ 0.1871, -0.4013, -0.0575,  ...,  0.2074,  0.0961, -0.1728],\n",
       "          ...,\n",
       "          [ 0.3415, -0.6743, -0.0978,  ...,  0.1450, -0.1007,  0.0544],\n",
       "          [ 0.0795, -0.6278, -0.0970,  ...,  0.0684, -0.0948, -0.0408],\n",
       "          [ 0.0864, -0.6242, -0.1237,  ..., -0.0317, -0.2537, -0.2492]],\n",
       "\n",
       "         [[-0.1728, -0.0534,  0.2756,  ...,  0.0490, -0.1544,  0.0092],\n",
       "          [-0.3238, -0.3740,  0.1887,  ..., -0.1150, -0.2522, -0.1836],\n",
       "          [-0.4228, -0.2138,  0.1466,  ...,  0.0235, -0.2765, -0.1056],\n",
       "          ...,\n",
       "          [ 0.1280,  0.0271, -0.0256,  ..., -0.1565, -0.1955, -0.4832],\n",
       "          [ 0.2738, -0.4181,  0.1941,  ..., -0.1985, -0.3031, -0.2569],\n",
       "          [ 0.0139, -0.2110, -0.0941,  ...,  0.1835, -0.3900, -0.4471]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>)), (tensor([[[[ 0.0870,  0.3587, -0.0013,  ..., -0.2935, -0.0628,  0.2759],\n",
       "          [-0.1152,  0.2125,  0.1462,  ..., -0.1888, -0.0713,  0.1111],\n",
       "          [ 0.1780, -0.1248, -0.1164,  ..., -0.1447,  0.0104,  0.1371],\n",
       "          ...,\n",
       "          [-0.1738, -0.0095,  0.0389,  ..., -0.1535,  0.0372,  0.0836],\n",
       "          [ 0.0095, -0.2398,  0.1117,  ..., -0.0767,  0.0116,  0.1625],\n",
       "          [ 0.0836, -0.2981, -0.0414,  ..., -0.0786,  0.1009,  0.1218]],\n",
       "\n",
       "         [[-0.1540,  0.0764,  0.4337,  ...,  0.4209,  0.0148,  0.1507],\n",
       "          [ 0.0463,  0.0635,  0.3608,  ...,  0.3113,  0.1680,  0.0060],\n",
       "          [ 0.0320, -0.0521,  0.2637,  ...,  0.5111,  0.0128,  0.0277],\n",
       "          ...,\n",
       "          [ 0.1995, -0.0634, -0.3071,  ...,  0.3185,  0.2326,  0.1457],\n",
       "          [ 0.0490, -0.0280, -0.0665,  ...,  0.3343,  0.2287,  0.0716],\n",
       "          [ 0.0370,  0.1366,  0.0709,  ...,  0.3334,  0.0460,  0.2290]],\n",
       "\n",
       "         [[-0.0437,  0.0090,  0.0595,  ...,  0.1218, -0.1574, -0.2031],\n",
       "          [ 0.0578, -0.0080, -0.0042,  ...,  0.0457, -0.1763, -0.0772],\n",
       "          [ 0.1455,  0.0313, -0.1355,  ..., -0.0569, -0.2003,  0.0035],\n",
       "          ...,\n",
       "          [-0.1064,  0.1287, -0.0206,  ...,  0.0902, -0.2218, -0.1833],\n",
       "          [ 0.1786,  0.3840,  0.0027,  ...,  0.0426, -0.2800, -0.2419],\n",
       "          [ 0.2039,  0.2447, -0.0567,  ..., -0.0583, -0.1697, -0.1327]],\n",
       "\n",
       "         [[ 0.3022, -0.0843, -0.3888,  ..., -0.2507, -0.0483, -0.1005],\n",
       "          [-0.1184, -0.2262, -0.2595,  ...,  0.0087, -0.0661, -0.1815],\n",
       "          [-0.3465, -0.0333, -0.0035,  ...,  0.0805, -0.1136, -0.3518],\n",
       "          ...,\n",
       "          [ 0.1015,  0.0583,  0.1346,  ..., -0.0690, -0.1486,  0.1129],\n",
       "          [-0.4329,  0.0671,  0.1563,  ..., -0.0302, -0.1827,  0.0684],\n",
       "          [-0.7378,  0.1987,  0.0833,  ..., -0.0960, -0.2302, -0.0413]]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>), tensor([[[[-0.3407, -0.1634, -0.0259,  ...,  0.3644, -0.1073,  0.1745],\n",
       "          [-0.1930, -0.2651,  0.0595,  ...,  0.2042,  0.2739,  0.1332],\n",
       "          [-0.2905, -0.1885,  0.1041,  ...,  0.0830,  0.1576,  0.0497],\n",
       "          ...,\n",
       "          [-0.0786, -0.1745,  0.0866,  ...,  0.2449,  0.5583, -0.1568],\n",
       "          [-0.0805, -0.0618,  0.0835,  ...,  0.1438,  0.4744, -0.0561],\n",
       "          [-0.1601, -0.1317, -0.0064,  ...,  0.1783,  0.4056, -0.0557]],\n",
       "\n",
       "         [[-0.3657, -0.2311, -0.1298,  ...,  0.5604,  0.0272, -0.0995],\n",
       "          [-0.1244, -0.1430,  0.0437,  ...,  0.4907,  0.0556, -0.0866],\n",
       "          [-0.3747, -0.1479,  0.1389,  ...,  0.5732, -0.0344, -0.0596],\n",
       "          ...,\n",
       "          [-0.2576, -0.3759,  0.0999,  ...,  0.3452,  0.0438, -0.0826],\n",
       "          [-0.0977, -0.4677,  0.1016,  ...,  0.2197, -0.0624, -0.0209],\n",
       "          [-0.1924, -0.4955,  0.2107,  ...,  0.3072,  0.0202, -0.1289]],\n",
       "\n",
       "         [[-0.0284,  0.0996,  0.2607,  ..., -0.4691,  0.0998,  0.2775],\n",
       "          [ 0.1298,  0.1846,  0.1657,  ..., -0.2677,  0.1338,  0.2926],\n",
       "          [-0.1584,  0.0204,  0.1200,  ..., -0.3217,  0.1651,  0.2245],\n",
       "          ...,\n",
       "          [ 0.0108,  0.1694,  0.3033,  ...,  0.0588,  0.0062,  0.1738],\n",
       "          [ 0.0348,  0.0719,  0.1010,  ...,  0.1646,  0.0683,  0.0416],\n",
       "          [ 0.0106,  0.0204,  0.0913,  ..., -0.1330, -0.0367,  0.1545]],\n",
       "\n",
       "         [[-0.0912, -0.1521,  0.1770,  ..., -0.1515, -0.2017,  0.0668],\n",
       "          [-0.2132,  0.1117,  0.1915,  ..., -0.1699,  0.0737,  0.1170],\n",
       "          [-0.1653,  0.0949,  0.1173,  ..., -0.1148, -0.1287,  0.3191],\n",
       "          ...,\n",
       "          [-0.3870,  0.1599,  0.1381,  ..., -0.0516, -0.0269,  0.1337],\n",
       "          [-0.3265,  0.2203,  0.0552,  ..., -0.0895,  0.0187,  0.3100],\n",
       "          [-0.3649,  0.1080,  0.0116,  ..., -0.0882, -0.1494,  0.4063]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>))), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LlamaConfig' object has no attribute 'use_fused_cel'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m out_fused \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_fused_cel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/notebooks/unsloth-dev/unsloth/kernels/fused_cel.py:63\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m     61\u001b[0m     logits \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_fused_cel\u001b[49m:\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing fused cross entropy loss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     65\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m/notebooks/conda/envs/unsloth/lib/python3.11/site-packages/transformers/configuration_utils.py:263\u001b[0m, in \u001b[0;36mPretrainedConfig.__getattribute__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattribute_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattribute_map\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    262\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattribute_map\u001b[39m\u001b[38;5;124m\"\u001b[39m)[key]\n\u001b[0;32m--> 263\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LlamaConfig' object has no attribute 'use_fused_cel'"
     ]
    }
   ],
   "source": [
    "out_fused = model.forward_fused_cel(\n",
    "    input_ids, labels=labels, attention_mask=attention_mask\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01c1bdc11e594a738ed0cc5415ff8fea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/51760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = data_utils.get_alpaca(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "max_seq_length = 4096\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=128,\n",
    "    gradient_accumulation_steps=1,\n",
    "    warmup_steps=5,\n",
    "    max_steps=5,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=not torch.cuda.is_bf16_supported(),\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    logging_steps=1,\n",
    "    optim=\"adamw_8bit\",\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=3407,\n",
    "    output_dir=\"outputs\",\n",
    "    # Metrics\n",
    "    skip_memory_metrics=False,\n",
    "    include_num_input_tokens_seen=True,\n",
    "    include_tokens_per_second=True,\n",
    ")\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model = model,\n",
    "#     tokenizer = tokenizer,\n",
    "#     train_dataset = dataset,\n",
    "#     dataset_text_field = \"text\",\n",
    "#     max_seq_length = max_seq_length,\n",
    "#     dataset_num_proc = 2,\n",
    "#     packing = False, # Can make training 5x faster for short sequences.\n",
    "#     args = training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16550e13227f45b396101c31d6e88574",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/51760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,  # Can make training 5x faster for short sequences.\n",
    "    args=training_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = trainer.get_train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = batch[\"input_ids\"]\n",
    "attention_mask = batch[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(107, device='cuda:0')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(input_ids[0] != 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(107, device='cuda:0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(attention_mask[0] != 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
