{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.3.0+cu121 with CUDA 1201 (you have 2.4.0.dev20240507+cu121)\n",
      "    Python  3.11.9 (you have 3.11.4)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "/home/jeromeku/.virtualenvs/work-env/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "./llama-10m does not have a padding token! Will use pad_token = <unk>.\n"
     ]
    }
   ],
   "source": [
    "import dataclasses\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from llama_head import CEL_only_forward\n",
    "from peft import LoraConfig\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    LlamaTokenizer,\n",
    "    Trainer,\n",
    "    TrainerCallback,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from transformers.models.llama import LlamaConfig, LlamaForCausalLM\n",
    "from transformers.trainer_callback import ProgressCallback\n",
    "from transformers.trainer_pt_utils import _secs2timedelta\n",
    "from trl import SFTTrainer\n",
    "\n",
    "import unsloth.utils.data as data_utils\n",
    "import unsloth.utils.memory as memory_utils\n",
    "import unsloth.utils.testing as test_utils\n",
    "from unsloth.kernels import fused_cel\n",
    "from unsloth.kernels.fused_cel import patch_model as patch_model_fused_cel\n",
    "from unsloth.models._utils import patch_tokenizer, prepare_model_for_kbit_training\n",
    "from unsloth.models.llama import FastLlamaModel\n",
    "from unsloth.utils.profiling import MetricsCallBack\n",
    "\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model_config = LlamaConfig.from_pretrained(\"./llama-10m.json\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"./llama-10m\", quantization_config=quant_config, torch_dtype=torch.bfloat16\n",
    ")\n",
    "# model = LlamaForCausalLM(model_config).to(\"cuda\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\", model_max_length=4096, padding_side=\"right\"\n",
    ")\n",
    "model, tokenizer = patch_tokenizer(model, tokenizer)\n",
    "\n",
    "max_seq_length = 256\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=1,\n",
    "    warmup_steps=5,\n",
    "    max_steps=5,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=not torch.cuda.is_bf16_supported(),\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    logging_steps=1,\n",
    "    optim=\"adamw_8bit\",\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=3407,\n",
    "    output_dir=\"outputs\",\n",
    "    overwrite_output_dir=True,\n",
    "    # Metrics\n",
    "    skip_memory_metrics=False,\n",
    "    include_num_input_tokens_seen=True,\n",
    "    include_tokens_per_second=True,\n",
    ")\n",
    "\n",
    "accepted_modules = frozenset(\n",
    "    (\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "dataset = data_utils.get_alpaca(tokenizer)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    target_modules=accepted_modules,\n",
    "    lora_alpha=8,\n",
    "    lora_dropout=0.0,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.utils.data import get_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d17c851d28a4b41b4f2b43fa4b8c791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataloader = get_data_loader(dataset, tokenizer, max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = list(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = batches[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:unsloth.kernels.fused_cel:Using fused cross entropy loss, output logits will be in None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(10.3927, grad_fn=<ToCopyBackward0>),\n",
       " tensor(10.3927, device='cuda:0',\n",
       "        grad_fn=<FusedCrossEntropyLossFunctionBackward>))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model(**batch)\n",
    "fused_out = patch_model_fused_cel(model, use_fused_cel=True)(**batch)\n",
    "out.loss, fused_out.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.3927, grad_fn=<ToCopyBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs, seqlen, in_features = 1, 16, 4096\n",
    "dtype = torch.bfloat16\n",
    "hidden_dim = model.config.hidden_size\n",
    "\n",
    "input_ids = torch.randint(0, model.config.vocab_size, (bs, seqlen), device=\"cuda\")\n",
    "hidden_states = torch.randn(\n",
    "    bs, seqlen, hidden_dim, dtype=dtype, device=\"cuda\", requires_grad=True\n",
    ")\n",
    "labels = input_ids.detach().clone()\n",
    "attention_mask = torch.ones((bs, seqlen), device=\"cuda\")\n",
    "\n",
    "# ref_out = model(input_ids, labels=labels, attention_mask=attention_mask)\n",
    "# ref_head = model.lm_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_out = model.model.embed_tokens(input_ids)\n",
    "decoder_out = model.model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_out = model.model(input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_out[\"last_hidden_state\"].requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to set inputs require_grad (i.e., output of embeddings requires grad for fused_cel to work)\n",
    "model = prepare_model_for_kbit_training(\n",
    "    model, use_gradient_checkpointing=training_args.gradient_checkpointing\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "patched_model = patch_model_fused_cel(model, use_fused_cel=True)\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=patched_model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,  # Can make training 5x faster for short sequences.\n",
    "    args=training_args,\n",
    ")\n",
    "# trainer.remove_callback(ProgressCallback)\n",
    "# _ = trainer.add_callback(MetricsCallBack())\n",
    "# train_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['output', 'input', 'instruction', 'text'],\n",
       "    num_rows: 51760\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = trainer.get_train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "batches = list(itertools.islice(train_loader, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1, 13866,   338,   385, 15278,   393, 16612,   263,  3414, 29892,\n",
       "          3300,  2859,   411,   385,  1881,   393,  8128,  4340,  3030, 29889,\n",
       "         14350,   263,  2933,   393,  7128,  2486,  1614,  2167,   278,  2009,\n",
       "         29889,    13,    13,  2277, 29937,  2799,  4080, 29901,    13, 29907,\n",
       "         20440,   675,   322,  3858,   278,  2183,  1426,   408,  2845,   263,\n",
       "          2114,   470,   385,  9426, 29889,    13,    13,  2277, 29937, 10567,\n",
       "         29901,    13,  1576, 14064,   471,  8031, 29889,    13,    13,  2277,\n",
       "         29937, 13291, 29901,    13, 11746,   262,   291, 29901,   450,  3229,\n",
       "           376,  1576, 14064,   471,  8031, 29908,   338,   385,  9426,  1363,\n",
       "           372,   338,   263,  4967,   573, 24284,  2729,   373,  7333, 21737,\n",
       "         29892, 21779, 29892,   470, 24583, 29889,     2,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0],\n",
       "        [    1, 13866,   338,   385, 15278,   393, 16612,   263,  3414, 29892,\n",
       "          3300,  2859,   411,   385,  1881,   393,  8128,  4340,  3030, 29889,\n",
       "         14350,   263,  2933,   393,  7128,  2486,  1614,  2167,   278,  2009,\n",
       "         29889,    13,    13,  2277, 29937,  2799,  4080, 29901,    13,  4391,\n",
       "           263, 24329, 20343,   411,   278,  1494,  2472,  1048,   278,  2183,\n",
       "          3234, 29889,    13,    13,  2277, 29937, 10567, 29901,    13, 29924,\n",
       "          4524,  3623,   625,    13,    13,  2277, 29937, 13291, 29901,    13,\n",
       "         29903,  7459, 29871, 29896, 29901,  3159,  3518,  3277,   341,  4524,\n",
       "          5342,   625,    13,    13, 29899,   341,  4524,  3623,   625,   338,\n",
       "           263,   628, 14803,   322,  2143,   690,  2790, 13748,  1754,   515,\n",
       "         10107,   412,   286,  4524,   267,    13, 29899,   910,  3623,   625,\n",
       "           338,  4870,   287,   411, 18853, 13901,   314,  1144,   322, 18254,\n",
       "           374,  1237, 29892,  3907,   372,   263,  9045, 29891,   367, 19698,\n",
       "          7348,    13,    13,    13, 29903,  7459, 29871, 29906, 29901, 15202,\n",
       "          4111,  1389,  1169,   310,   341,  4524,  5342,   625,    13,    13,\n",
       "         29899,  2866,  2708,  1880, 11174,   310, 13901,  9103,   315, 29892,\n",
       "           607, 14505, 29879,   278,  5198,  1540,  1788,    13, 29899,  4385,\n",
       "           297, 13901,  9103,   319, 29892,   607,   338, 18853,   363, 10977,\n",
       "          9045,    13, 29899, 18744,   287,   411,  3677,   601, 29916,   333,\n",
       "          1934,   393,  1371, 12566,  2750,  3038, 18658,    13, 29899,  7197,\n",
       "          2752,   310,   652,   300,   653,  5713,   495, 29892,   607,   263,\n",
       "          4841,   297,  4697,   602,    13,    13,    13, 29903,  7459, 29871,\n",
       "         29941, 29901,  1128,   304,  1174,  2212, 29891,   341,  4524,  5342,\n",
       "           625,    13,    13, 29899,  1174,  2212, 29891,   263, 12917,   310,\n",
       "           521, 24455,   286,  4524,  3623,   625,   408,   263,  2143,   690,\n",
       "          2790, 13748,   373,   263,  7375,  2462]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0'), 'labels': tensor([[    1, 13866,   338,   385, 15278,   393, 16612,   263,  3414, 29892,\n",
       "          3300,  2859,   411,   385,  1881,   393,  8128,  4340,  3030, 29889,\n",
       "         14350,   263,  2933,   393,  7128,  2486,  1614,  2167,   278,  2009,\n",
       "         29889,    13,    13,  2277, 29937,  2799,  4080, 29901,    13, 29907,\n",
       "         20440,   675,   322,  3858,   278,  2183,  1426,   408,  2845,   263,\n",
       "          2114,   470,   385,  9426, 29889,    13,    13,  2277, 29937, 10567,\n",
       "         29901,    13,  1576, 14064,   471,  8031, 29889,    13,    13,  2277,\n",
       "         29937, 13291, 29901,    13, 11746,   262,   291, 29901,   450,  3229,\n",
       "           376,  1576, 14064,   471,  8031, 29908,   338,   385,  9426,  1363,\n",
       "           372,   338,   263,  4967,   573, 24284,  2729,   373,  7333, 21737,\n",
       "         29892, 21779, 29892,   470, 24583, 29889,     2,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100],\n",
       "        [    1, 13866,   338,   385, 15278,   393, 16612,   263,  3414, 29892,\n",
       "          3300,  2859,   411,   385,  1881,   393,  8128,  4340,  3030, 29889,\n",
       "         14350,   263,  2933,   393,  7128,  2486,  1614,  2167,   278,  2009,\n",
       "         29889,    13,    13,  2277, 29937,  2799,  4080, 29901,    13,  4391,\n",
       "           263, 24329, 20343,   411,   278,  1494,  2472,  1048,   278,  2183,\n",
       "          3234, 29889,    13,    13,  2277, 29937, 10567, 29901,    13, 29924,\n",
       "          4524,  3623,   625,    13,    13,  2277, 29937, 13291, 29901,    13,\n",
       "         29903,  7459, 29871, 29896, 29901,  3159,  3518,  3277,   341,  4524,\n",
       "          5342,   625,    13,    13, 29899,   341,  4524,  3623,   625,   338,\n",
       "           263,   628, 14803,   322,  2143,   690,  2790, 13748,  1754,   515,\n",
       "         10107,   412,   286,  4524,   267,    13, 29899,   910,  3623,   625,\n",
       "           338,  4870,   287,   411, 18853, 13901,   314,  1144,   322, 18254,\n",
       "           374,  1237, 29892,  3907,   372,   263,  9045, 29891,   367, 19698,\n",
       "          7348,    13,    13,    13, 29903,  7459, 29871, 29906, 29901, 15202,\n",
       "          4111,  1389,  1169,   310,   341,  4524,  5342,   625,    13,    13,\n",
       "         29899,  2866,  2708,  1880, 11174,   310, 13901,  9103,   315, 29892,\n",
       "           607, 14505, 29879,   278,  5198,  1540,  1788,    13, 29899,  4385,\n",
       "           297, 13901,  9103,   319, 29892,   607,   338, 18853,   363, 10977,\n",
       "          9045,    13, 29899, 18744,   287,   411,  3677,   601, 29916,   333,\n",
       "          1934,   393,  1371, 12566,  2750,  3038, 18658,    13, 29899,  7197,\n",
       "          2752,   310,   652,   300,   653,  5713,   495, 29892,   607,   263,\n",
       "          4841,   297,  4697,   602,    13,    13,    13, 29903,  7459, 29871,\n",
       "         29941, 29901,  1128,   304,  1174,  2212, 29891,   341,  4524,  5342,\n",
       "           625,    13,    13, 29899,  1174,  2212, 29891,   263, 12917,   310,\n",
       "           521, 24455,   286,  4524,  3623,   625,   408,   263,  2143,   690,\n",
       "          2790, 13748,   373,   263,  7375,  2462]], device='cuda:0')}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batches = [b for b in accepted_modules\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
