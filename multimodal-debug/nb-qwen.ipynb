{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruff: noqa\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "REPO_ROOT = Path(\"..\").absolute()\n",
    "sys.path.append(str(REPO_ROOT))\n",
    "\n",
    "from contextlib import contextmanager\n",
    "from unsloth import FastVisionModel  # FastLanguageModel for LLMs\n",
    "from unsloth.trainer import UnslothVisionDataCollator\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import TextStreamer\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from IPython.display import display, Math, Latex\n",
    "\n",
    "MODEL_NAME = \"unsloth/Qwen2-VL-7B-Instruct\"\n",
    "DATASET_NAME = \"unsloth/LaTeX_OCR\"\n",
    "\n",
    "INSTRUCTION = \"Write the LaTeX representation for this image.\"\n",
    "\n",
    "FINE_TUNE_CONFIG = {\n",
    "    \"finetune_vision_layers\": True,\n",
    "    \"finetune_language_layers\": True,\n",
    "    \"finetune_attention_modules\": True,\n",
    "    \"finetune_mlp_modules\": True,\n",
    "}\n",
    "LORA_CONFIG = {\n",
    "    \"r\": 16,\n",
    "    \"lora_alpha\": 16,\n",
    "    \"lora_dropout\": 0,\n",
    "    \"bias\": \"none\",\n",
    "    \"use_rslora\": False,\n",
    "    \"loftq_config\": None,\n",
    "}\n",
    "DTYPE = torch.bfloat16\n",
    "TRAIN_CONFIG = {\n",
    "    \"per_device_train_batch_size\": 2,\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"warmup_steps\": 5,\n",
    "    \"max_steps\": 30,\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"fp16\": DTYPE == torch.float16,\n",
    "    \"bf16\": DTYPE == torch.bfloat16,\n",
    "    \"optim\": \"adamw_8bit\",\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"lr_scheduler_type\": \"linear\",\n",
    "    \"seed\": 3407,\n",
    "}\n",
    "\n",
    "LOG_CONFIG = {\n",
    "    \"logging_steps\": 1,\n",
    "    \"output_dir\": \"qwen-vl-outputs\",\n",
    "    \"report_to\": \"none\",\n",
    "}\n",
    "\n",
    "DATASET_CONFIG = {\n",
    "    \"remove_unused_columns\": False,\n",
    "    \"dataset_text_field\": \"\",\n",
    "    \"dataset_kwargs\": {\"skip_prepare_dataset\": True},\n",
    "    \"dataset_num_proc\": 4,\n",
    "    \"max_seq_length\": 2048,\n",
    "}\n",
    "\n",
    "SAVE_PATH = \"qwen_vl_lora_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model_and_tokenizer(\n",
    "    model_name,\n",
    "    fine_tune_config: dict,\n",
    "    lora_config: dict,\n",
    "    load_in_4bit=True,\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    "    **kwargs,\n",
    "):\n",
    "    model, tokenizer = FastVisionModel.from_pretrained(\n",
    "        model_name,\n",
    "        load_in_4bit=load_in_4bit,  # Use 4bit to reduce memory use. False for 16bit LoRA.\n",
    "        use_gradient_checkpointing=use_gradient_checkpointing,  # True or \"unsloth\" for long context\n",
    "    )\n",
    "\n",
    "    model = FastVisionModel.get_peft_model(\n",
    "        model,\n",
    "        **fine_tune_config,\n",
    "        **lora_config,\n",
    "        random_state=random_state,\n",
    "        # target_modules = \"all-linear\", # Optional now! Can specify a list if needed\n",
    "    )\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def prepare_dataset(dataset_name, split=\"train\"):\n",
    "    dataset = load_dataset(dataset_name, split=split)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def convert_to_conversation(sample, instruction=INSTRUCTION):\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": instruction},\n",
    "                {\"type\": \"image\", \"image\": sample[\"image\"]},\n",
    "            ],\n",
    "        },\n",
    "        {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": sample[\"text\"]}]},\n",
    "    ]\n",
    "    return {\"messages\": conversation}\n",
    "\n",
    "\n",
    "def generate_image_text(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    image,\n",
    "    instruction=INSTRUCTION,\n",
    "    temperature=1.5,\n",
    "    min_p=0.1,\n",
    "    max_new_tokens=128,\n",
    "    use_cache=True,\n",
    "):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": instruction}],\n",
    "        }\n",
    "    ]\n",
    "    input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
    "    inputs = tokenizer(\n",
    "        image,\n",
    "        input_text,\n",
    "        add_special_tokens=False,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "    return model.generate(\n",
    "        **inputs,\n",
    "        streamer=text_streamer,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        use_cache=use_cache,\n",
    "        temperature=temperature,\n",
    "        min_p=min_p,\n",
    "    )\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def inference_context(model):\n",
    "    FastVisionModel.for_inference(model)\n",
    "    yield\n",
    "    FastVisionModel.for_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.17: Fast Qwen2 patching. Transformers: 4.49.0. vLLM: 0.7.3.\n",
      "   \\\\   /|    NVIDIA H100 80GB HBM3. Num GPUs = 1. Max memory: 79.109 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 9.0. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Making `model.base_model.model.visual` require gradients\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Math' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m image = dataset[\u001b[32m2\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mimage\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      7\u001b[39m latex = dataset[\u001b[32m2\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m display(\u001b[43mMath\u001b[49m(latex))\n",
      "\u001b[31mNameError\u001b[39m: name 'Math' is not defined"
     ]
    }
   ],
   "source": [
    "model, tokenizer = prepare_model_and_tokenizer(\n",
    "    MODEL_NAME, FINE_TUNE_CONFIG, LORA_CONFIG\n",
    ")\n",
    "dataset = prepare_dataset(DATASET_NAME)\n",
    "converted_dataset = [convert_to_conversation(sample) for sample in dataset]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle H ^ { \\prime } = \\beta N \\int d \\lambda \\biggl \\{ \\frac { 1 } { 2 \\beta ^ { 2 } N ^ { 2 } } \\partial _ { \\lambda } \\zeta ^ { \\dagger } \\partial _ { \\lambda } \\zeta + V ( \\lambda ) \\zeta ^ { \\dagger } \\zeta \\biggr \\} \\ .$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "image = dataset[2][\"image\"]\n",
    "latex = dataset[2][\"text\"]\n",
    "display(Math(latex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_name = \"Qwen/Qwen2-VL-7B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=UnslothVisionDataCollator(model, tokenizer),  # Must use!\n",
    "    train_dataset=converted_dataset,\n",
    "    args=SFTConfig(\n",
    "        **TRAIN_CONFIG,\n",
    "        **LOG_CONFIG,\n",
    "        **DATASET_CONFIG,\n",
    "    ),\n",
    ")\n",
    "\n",
    "with inference_context(model):\n",
    "    outputs = generate_image_text(model, tokenizer, image, instruction=INSTRUCTION)\n",
    "    print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".unsloth.env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
