{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "DEBUG::VLLLM VLLM_LOGGING_CONFIG_PATH None\n",
      "INFO 03-22 11:35:21 /home/jeromeku/dev/third_party/unsloth/.unsloth.env/lib/python3.11/site-packages/vllm/platforms/__init__.py:207] Automatically detected platform cuda.\n",
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "DEBUG::VLLLM VLLM_LOGGING_CONFIG_PATH None\n",
      "INFO 03-22 11:35:27 /home/jeromeku/dev/third_party/unsloth/.unsloth.env/lib/python3.11/site-packages/vllm/platforms/__init__.py:207] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "# ruff: noqa\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "REPO_ROOT = Path(\"..\").absolute()\n",
    "sys.path.append(str(REPO_ROOT))\n",
    "\n",
    "from contextlib import contextmanager\n",
    "from unsloth import FastVisionModel  # FastLanguageModel for LLMs\n",
    "from unsloth.trainer import UnslothVisionDataCollator\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import TextStreamer\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from IPython.display import display, Math, Latex\n",
    "\n",
    "MODEL_NAME = \"unsloth/Qwen2-VL-7B-Instruct\"\n",
    "DATASET_NAME = \"unsloth/LaTeX_OCR\"\n",
    "\n",
    "INSTRUCTION = \"Write the LaTeX representation for this image.\"\n",
    "\n",
    "FINE_TUNE_CONFIG = {\n",
    "    \"finetune_vision_layers\": True,\n",
    "    \"finetune_language_layers\": True,\n",
    "    \"finetune_attention_modules\": True,\n",
    "    \"finetune_mlp_modules\": True,\n",
    "}\n",
    "LORA_CONFIG = {\n",
    "    \"r\": 16,\n",
    "    \"lora_alpha\": 16,\n",
    "    \"lora_dropout\": 0,\n",
    "    \"bias\": \"none\",\n",
    "    \"use_rslora\": False,\n",
    "    \"loftq_config\": None,\n",
    "}\n",
    "DTYPE = torch.bfloat16\n",
    "TRAIN_CONFIG = {\n",
    "    \"per_device_train_batch_size\": 2,\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"warmup_steps\": 5,\n",
    "    \"max_steps\": 30,\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"fp16\": DTYPE == torch.float16,\n",
    "    \"bf16\": DTYPE == torch.bfloat16,\n",
    "    \"optim\": \"adamw_8bit\",\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"lr_scheduler_type\": \"linear\",\n",
    "    \"seed\": 3407,\n",
    "}\n",
    "\n",
    "LOG_CONFIG = {\n",
    "    \"logging_steps\": 1,\n",
    "    \"output_dir\": \"qwen-vl-outputs\",\n",
    "    \"report_to\": \"none\",\n",
    "}\n",
    "\n",
    "DATASET_CONFIG = {\n",
    "    \"remove_unused_columns\": False,\n",
    "    \"dataset_text_field\": \"\",\n",
    "    \"dataset_kwargs\": {\"skip_prepare_dataset\": True},\n",
    "    \"dataset_num_proc\": 4,\n",
    "    \"max_seq_length\": 2048,\n",
    "}\n",
    "\n",
    "SAVE_PATH = \"qwen_vl_lora_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model_and_tokenizer(\n",
    "    model_name,\n",
    "    fine_tune_config: dict,\n",
    "    lora_config: dict,\n",
    "    load_in_4bit=True,\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    "    **kwargs,\n",
    "):\n",
    "    model, tokenizer = FastVisionModel.from_pretrained(\n",
    "        model_name,\n",
    "        load_in_4bit=load_in_4bit,  # Use 4bit to reduce memory use. False for 16bit LoRA.\n",
    "        use_gradient_checkpointing=use_gradient_checkpointing,  # True or \"unsloth\" for long context\n",
    "    )\n",
    "\n",
    "    model = FastVisionModel.get_peft_model(\n",
    "        model,\n",
    "        **fine_tune_config,\n",
    "        **lora_config,\n",
    "        random_state=random_state,\n",
    "        # target_modules = \"all-linear\", # Optional now! Can specify a list if needed\n",
    "    )\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def prepare_dataset(dataset_name, split=\"train\"):\n",
    "    dataset = load_dataset(dataset_name, split=split)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def convert_to_conversation(sample, instruction=INSTRUCTION):\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": instruction},\n",
    "                {\"type\": \"image\", \"image\": sample[\"image\"]},\n",
    "            ],\n",
    "        },\n",
    "        {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": sample[\"text\"]}]},\n",
    "    ]\n",
    "    return {\"messages\": conversation}\n",
    "\n",
    "\n",
    "def generate_image_text(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    image,\n",
    "    instruction=INSTRUCTION,\n",
    "    temperature=1.5,\n",
    "    min_p=0.1,\n",
    "    max_new_tokens=128,\n",
    "    use_cache=True,\n",
    "):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": instruction}],\n",
    "        }\n",
    "    ]\n",
    "    input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
    "    inputs = tokenizer(\n",
    "        image,\n",
    "        input_text,\n",
    "        add_special_tokens=False,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "    return model.generate(\n",
    "        **inputs,\n",
    "        streamer=text_streamer,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        use_cache=use_cache,\n",
    "        temperature=temperature,\n",
    "        min_p=min_p,\n",
    "    )\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def inference_context(model):\n",
    "    FastVisionModel.for_inference(model)\n",
    "    yield\n",
    "    FastVisionModel.for_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.17: Fast Qwen2 patching. Transformers: 4.49.0. vLLM: 0.7.3.\n",
      "   \\\\   /|    NVIDIA H100 80GB HBM3. Num GPUs = 1. Max memory: 79.109 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 9.0. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.visual` require gradients\n",
      "Unsloth: Model does not have a default image size - using 512\n",
      "$$H ^ { \\prime } = \\: \\::::\n",
      ":::\n",
      ":\n",
      "\n",
      "<|im_start|>:::-template\n",
      "\n",
      ":::\n",
      ":-:::\n",
      "\n",
      "::-::\n",
      "\n",
      "::\n",
      ":\n",
      ":-::\n",
      ":\n",
      ":-::\n",
      ":\n",
      ":-::\n",
      ":\n",
      ":(\"\"+system:::\n",
      ":(\"\"+:::-::(\"\"+::,<|im_end|>\n",
      "tensor([[151644,   8948,    198,   2610,    525,    264,  10950,  17847,     13,\n",
      "         151645,    198, 151644,    872,    198, 151652, 151655, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151655, 151653,   7985,    279,  97913,  13042,    369,    419,   2168,\n",
      "             13, 151645,    198, 151644,  77091,    198,  14085,     39,   6306,\n",
      "            314,   1124,  32338,    335,    284,   1124,     25,   1124,     25,\n",
      "             25,     25,     25,    198,     25,     25,     25,    198,     25,\n",
      "            271, 151644,     25,     25,     25,     12,   4214,    271,     25,\n",
      "             25,     25,    198,     25,     12,     25,     25,     25,    271,\n",
      "             25,     25,     12,     25,     25,    271,     25,     25,    198,\n",
      "             25,    198,     25,     12,     25,     25,    198,     25,    198,\n",
      "             25,     12,     25,     25,    198,     25,    198,     25,     12,\n",
      "             25,     25,    198,     25,    198,     25,  97470,   8948,     25,\n",
      "             25,     25,    198,     25,  97470,     25,     25,     25,     12,\n",
      "             25,     25,  97470,     25,     25,     11, 151645]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = prepare_model_and_tokenizer(\n",
    "    MODEL_NAME, FINE_TUNE_CONFIG, LORA_CONFIG\n",
    ")\n",
    "dataset = prepare_dataset(DATASET_NAME)\n",
    "converted_dataset = [convert_to_conversation(sample) for sample in dataset]\n",
    "image = dataset[2][\"image\"]\n",
    "latex = dataset[2][\"text\"]\n",
    "display(Math(latex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=UnslothVisionDataCollator(model, tokenizer),  # Must use!\n",
    "    train_dataset=converted_dataset,\n",
    "    args=SFTConfig(\n",
    "        **TRAIN_CONFIG,\n",
    "        **LOG_CONFIG,\n",
    "        **DATASET_CONFIG,\n",
    "    ),\n",
    ")\n",
    "\n",
    "with inference_context(model):\n",
    "    outputs = generate_image_text(model, tokenizer, image, instruction=INSTRUCTION)\n",
    "    print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".unsloth.env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
