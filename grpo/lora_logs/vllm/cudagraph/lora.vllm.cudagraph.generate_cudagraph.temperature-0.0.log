USE_CUDAGRAPH: True
ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.
ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!
DEBUG::VLLLM VLLM_LOGGING_CONFIG_PATH None
Using LoRA model...
INFO 03-17 19:52:41 /home/jeromeku/dev/third_party/unsloth/.unsloth.env/lib/python3.11/site-packages/vllm/platforms/__init__.py:207] Automatically detected platform cuda.
==((====))==  Unsloth 2025.3.14: Fast Qwen2 patching. Transformers: 4.49.0. vLLM: 0.7.3.
   \\   /|    NVIDIA H100 80GB HBM3. Num GPUs = 1. Max memory: 79.109 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.5.1+cu124. CUDA: 9.0. CUDA Toolkit: 12.4. Triton: 3.1.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: vLLM loading unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit with actual GPU utilization = 49.62%
Unsloth: Your GPU has CUDA compute capability 9.0 with VRAM = 79.11 GB.
Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 1024. Num Sequences = 320.
Unsloth: vLLM's KV Cache can use up to 36.84 GB. Also swap space = 6 GB.
INFO 03-17 19:52:48 /home/jeromeku/dev/third_party/unsloth/.unsloth.env/lib/python3.11/site-packages/vllm/config.py:549] This model supports multiple tasks: {'classify', 'generate', 'reward', 'score', 'embed'}. Defaulting to 'generate'.
Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'model.layers.2.mlp', 'model.layers.3.mlp', 'model.layers.30.mlp'], 'llm_int8_threshold': 6.0}
INFO 03-17 19:52:48 /home/jeromeku/dev/third_party/unsloth/.unsloth.env/lib/python3.11/site-packages/vllm/engine/llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"level":0,"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":320}, use_cached_outputs=False, 
INFO 03-17 19:52:49 /home/jeromeku/dev/third_party/unsloth/.unsloth.env/lib/python3.11/site-packages/vllm/platforms/cuda.py:229] Using Flash Attention backend.
[W317 19:52:49.634161125 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
INFO 03-17 19:52:49 /home/jeromeku/dev/third_party/unsloth/.unsloth.env/lib/python3.11/site-packages/vllm/worker/model_runner.py:1110] Starting to load model unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit...
INFO 03-17 19:52:49 /home/jeromeku/dev/third_party/unsloth/.unsloth.env/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py:1089] Loading weights with BitsAndBytes quantization.  May take a while ...
INFO 03-17 19:52:50 /home/jeromeku/dev/third_party/unsloth/.unsloth.env/lib/python3.11/site-packages/vllm/model_executor/model_loader/weight_utils.py:254] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.19it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.19it/s]

Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.02it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.02it/s]

INFO 03-17 19:52:51 /home/jeromeku/dev/third_party/unsloth/.unsloth.env/lib/python3.11/site-packages/vllm/worker/model_runner.py:1115] Loading model weights took 2.2160 GB
INFO 03-17 19:52:51 /home/jeromeku/dev/third_party/unsloth/.unsloth.env/lib/python3.11/site-packages/vllm/lora/punica_wrapper/punica_selector.py:18] Using PunicaWrapperGPU.
INFO 03-17 19:52:53 /home/jeromeku/dev/third_party/unsloth/.unsloth.env/lib/python3.11/site-packages/vllm/worker/worker.py:267] Memory profiling takes 1.66 seconds
INFO 03-17 19:52:53 /home/jeromeku/dev/third_party/unsloth/.unsloth.env/lib/python3.11/site-packages/vllm/worker/worker.py:267] the current vLLM instance can use total_gpu_memory (79.11GiB) x gpu_memory_utilization (0.50) = 39.25GiB
INFO 03-17 19:52:53 /home/jeromeku/dev/third_party/unsloth/.unsloth.env/lib/python3.11/site-packages/vllm/worker/worker.py:267] model weights take 2.22GiB; non_torch_memory takes 0.15GiB; PyTorch activation peak memory takes 1.77GiB; the rest of the memory reserved for KV Cache is 35.12GiB.
INFO 03-17 19:52:53 /home/jeromeku/dev/third_party/unsloth/.unsloth.env/lib/python3.11/site-packages/vllm/executor/executor_base.py:111] # cuda blocks: 63926, # CPU blocks: 10922
INFO 03-17 19:52:53 /home/jeromeku/dev/third_party/unsloth/.unsloth.env/lib/python3.11/site-packages/vllm/executor/executor_base.py:116] Maximum concurrency for 1024 tokens per request: 998.84x
INFO 03-17 19:52:56 /home/jeromeku/dev/third_party/unsloth/.unsloth.env/lib/python3.11/site-packages/vllm/worker/model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/43 [00:00<?, ?it/s]Capturing CUDA graph shapes:   2%|â–         | 1/43 [00:00<00:29,  1.43it/s]Capturing CUDA graph shapes:   5%|â–         | 2/43 [00:01<00:27,  1.48it/s]Capturing CUDA graph shapes:   7%|â–‹         | 3/43 [00:02<00:37,  1.08it/s]Capturing CUDA graph shapes:   9%|â–‰         | 4/43 [00:04<00:44,  1.13s/it]Capturing CUDA graph shapes:  12%|â–ˆâ–        | 5/43 [00:05<00:48,  1.28s/it]Capturing CUDA graph shapes:  14%|â–ˆâ–        | 6/43 [00:07<00:51,  1.39s/it]Capturing CUDA graph shapes:  16%|â–ˆâ–‹        | 7/43 [00:08<00:53,  1.48s/it]Capturing CUDA graph shapes:  19%|â–ˆâ–Š        | 8/43 [00:09<00:41,  1.19s/it]Capturing CUDA graph shapes:  21%|â–ˆâ–ˆ        | 9/43 [00:09<00:33,  1.01it/s]Capturing CUDA graph shapes:  23%|â–ˆâ–ˆâ–Ž       | 10/43 [00:10<00:29,  1.13it/s]Capturing CUDA graph shapes:  26%|â–ˆâ–ˆâ–Œ       | 11/43 [00:11<00:25,  1.28it/s]Capturing CUDA graph shapes:  28%|â–ˆâ–ˆâ–Š       | 12/43 [00:11<00:22,  1.37it/s]Capturing CUDA graph shapes:  30%|â–ˆâ–ˆâ–ˆ       | 13/43 [00:12<00:20,  1.43it/s]Capturing CUDA graph shapes:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 14/43 [00:12<00:18,  1.53it/s]Capturing CUDA graph shapes:  35%|â–ˆâ–ˆâ–ˆâ–      | 15/43 [00:13<00:19,  1.43it/s]Capturing CUDA graph shapes:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 16/43 [00:14<00:18,  1.50it/s]Capturing CUDA graph shapes:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 17/43 [00:14<00:16,  1.54it/s]Capturing CUDA graph shapes:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 18/43 [00:15<00:15,  1.61it/s]Capturing CUDA graph shapes:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 19/43 [00:16<00:15,  1.59it/s]Capturing CUDA graph shapes:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 20/43 [00:16<00:14,  1.61it/s]Capturing CUDA graph shapes:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 21/43 [00:17<00:13,  1.61it/s]Capturing CUDA graph shapes:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 22/43 [00:17<00:12,  1.62it/s]Capturing CUDA graph shapes:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 23/43 [00:18<00:12,  1.66it/s]Capturing CUDA graph shapes:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 24/43 [00:19<00:11,  1.70it/s]Capturing CUDA graph shapes:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 25/43 [00:19<00:10,  1.68it/s]Capturing CUDA graph shapes:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 26/43 [00:20<00:11,  1.48it/s]Capturing CUDA graph shapes:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 27/43 [00:22<00:14,  1.08it/s]Capturing CUDA graph shapes:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 28/43 [00:22<00:12,  1.24it/s]Capturing CUDA graph shapes:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 29/43 [00:23<00:10,  1.32it/s]Capturing CUDA graph shapes:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 30/43 [00:23<00:09,  1.39it/s]Capturing CUDA graph shapes:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 31/43 [00:24<00:08,  1.48it/s]Capturing CUDA graph shapes:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 32/43 [00:25<00:07,  1.51it/s]Capturing CUDA graph shapes:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 33/43 [00:25<00:06,  1.58it/s]Capturing CUDA graph shapes:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 34/43 [00:26<00:05,  1.59it/s]Capturing CUDA graph shapes:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 35/43 [00:26<00:05,  1.58it/s]Capturing CUDA graph shapes:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 36/43 [00:27<00:04,  1.63it/s]Capturing CUDA graph shapes:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 37/43 [00:28<00:03,  1.59it/s]Capturing CUDA graph shapes:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 38/43 [00:28<00:03,  1.65it/s]Capturing CUDA graph shapes:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 39/43 [00:29<00:02,  1.73it/s]Capturing CUDA graph shapes:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 40/43 [00:29<00:01,  1.72it/s]Capturing CUDA graph shapes:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 41/43 [00:30<00:01,  1.57it/s]Capturing CUDA graph shapes:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 42/43 [00:31<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [00:31<00:00,  1.63it/s]Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [00:31<00:00,  1.35it/s]
INFO 03-17 19:53:28 /home/jeromeku/dev/third_party/unsloth/.unsloth.env/lib/python3.11/site-packages/vllm/worker/model_runner.py:1562] Graph capturing finished in 32 secs, took 6.28 GiB
INFO 03-17 19:53:28 /home/jeromeku/dev/third_party/unsloth/.unsloth.env/lib/python3.11/site-packages/vllm/engine/llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 36.91 seconds
Unsloth 2025.3.14 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.
Loading saved adapter weights...
Active adapters  ['default']
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.71it/s, est. speed input: 73.57 toks/s, output: 116.34 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.71it/s, est. speed input: 73.57 toks/s, output: 116.34 toks/s]
Without lora: <reasoning>
To determine how many times the letter 'r' appears in the word "strawberry", I will go through the word character by character and count each occurrence of 'r'.
</reasoning>
<answer>
The letter 'r' appears 3 times in the word "strawberry".
</answer>
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.40it/s, est. speed input: 60.18 toks/s, output: 95.16 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.40it/s, est. speed input: 60.18 toks/s, output: 95.16 toks/s]
With lora: <reasoning>
To determine how many times the letter 'r' appears in the word "strawberry", I will go through the word character by character and count each occurrence of 'r'.
</reasoning>
<answer>
The letter 'r' appears 3 times in the word "strawberry".
</answer>
[rank0]:[W317 19:53:47.163022660 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
